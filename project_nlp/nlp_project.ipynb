{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* the pdf with your answers\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Python 3.6 or above is required\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('data/')\n",
    "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
    "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
    "if not en_embeddings_path.exists():\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
    "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
    "if not fr_embeddings_path.exists():\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "\n",
    "    def __init__(self, filepath, vocab_size=50000):\n",
    "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
    "        # Mappings for O(1) retrieval:\n",
    "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
    "    \n",
    "    def load_wordvec(self, filepath, vocab_size):\n",
    "        assert str(filepath).endswith('.gz')\n",
    "        words = []\n",
    "        embeddings = []\n",
    "        with gzip.open(filepath, 'rt') as f:  # Read compressed file directly\n",
    "            next(f)  # Skip header\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                words.append(word)\n",
    "                embeddings.append(np.fromstring(vec, sep=' '))\n",
    "                if i == (vocab_size - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
    "        return words, np.vstack(embeddings)\n",
    "    \n",
    "    def encode(self, word):\n",
    "        # Returns the 1D embedding of a given word\n",
    "    \n",
    "        idx = self.word2id[word]  # Index of the given word\n",
    "    \n",
    "        return self.embeddings[idx]  \n",
    "        \n",
    "    \n",
    "    def score(self, word1, word2):\n",
    "        # Returns the cosine similarity: use np.dot & np.linalg.norm\n",
    "        # Get embedding of both words\n",
    "        embed1 = self.encode(word1)\n",
    "        embed2 = self.encode(word2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = np.dot(embed1,embed2) / (np.linalg.norm(embed1)*np.linalg.norm(embed2))  \n",
    "        \n",
    "        return cosine_similarity\n",
    "    \n",
    "    def most_similar(self, word, k=5):\n",
    "        # Return the k most similar vector embeddings to the vector embedding of 'word'\n",
    "        scores = []  \n",
    "        \n",
    "        # Build a list of cosine similarities between the given word and the other words in the vocabulary\n",
    "        for word_prime, index in self.word2id.items():      \n",
    "            scores.append(self.score(word,word_prime))      # Note: Includes the cosine similarity with itself\n",
    "        \n",
    "        # Returns the indices that would sort the list of scores in the descending order\n",
    "        scores = np.array(scores)  \n",
    "        scores_index_sorted = np.argsort(-scores)    \n",
    "        \n",
    "        # k most similar vector embeddings, from most similar to least similar\n",
    "        k_most_similar = [self.id2word[scores_index_sorted[i]] for i in range(1,k+1)]\n",
    "        \n",
    "        return k_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "cat tree 0.26449754661654756\n",
      "cat dog 0.7078641298542564\n",
      "cat pet 0.6753313359976382\n",
      "Paris France 0.6892958925806543\n",
      "Paris Germany 0.4051242286737549\n",
      "Paris baguette 0.29399958277802224\n",
      "Paris donut -0.006588507552348003\n",
      "['cats', 'kitty', 'kitten', 'feline', 'dog']\n",
      "['dogs', 'puppy', 'pup', 'canine', 'pet']\n",
      "['dog', 'cats', 'puppies', 'Dogs', 'pets']\n",
      "['France', 'Parisian', 'Marseille', 'Brussels', 'Strasbourg']\n",
      "['Austria', 'Europe', 'Berlin', 'Hamburg', 'Bavaria']\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
    "    print(word1, word2, word2vec.score(word1, word2))\n",
    "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
    "    print(word2vec.most_similar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BagOfWords():\n",
    "    \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # -> idf = {word: idf_value, ...}\n",
    "        num_sentences = len(sentences)\n",
    "        word2idf = {}\n",
    "        \n",
    "        # Compute the number of sentences containing the word, associated to each word\n",
    "        for word, index in self.word2vec.word2id.items():\n",
    "            word2idf[word] = 0\n",
    "            for sentence in sentences:\n",
    "                if word in sentence:\n",
    "                    word2idf[word] += 1\n",
    "        \n",
    "        # Compute the Inverse Document Frequency IDF\n",
    "        for word, value in word2idf.items():\n",
    "            if word2idf[word] > 0:\n",
    "                word2idf[word] = np.log(num_sentences / word2idf[word])\n",
    "        \n",
    "        return word2idf\n",
    "    \n",
    "    def encode(self, sentence, idf=None):\n",
    "        # Takes a sentence as input, returns the sentence embedding\n",
    "        sentence = sentence.split(' ') # Convert sentence to list of words\n",
    "        sentence_vocab = [] \n",
    "        mean = 0\n",
    "        \n",
    "        # Build the list of words in the sentence also contained in the vocabulary\n",
    "        for word in sentence:\n",
    "            if word in self.word2vec.words:\n",
    "                sentence_vocab.append(word)\n",
    "        \n",
    "        if len(sentence_vocab) == 0:\n",
    "            return mean\n",
    "        \n",
    "        else:\n",
    "            if idf is None:\n",
    "                # mean of word vectors\n",
    "                # Compute mean of word embeddings\n",
    "                for word in sentence_vocab:\n",
    "                        mean += word2vec.encode(word)\n",
    "                \n",
    "                sentence_embed = mean / len(sentence_vocab)\n",
    "        \n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                for word in sentence_vocab:\n",
    "                    mean += idf[word]*word2vec.encode(word)\n",
    "            \n",
    "                sentence_embed = mean / len(sentence_vocab)\n",
    "            \n",
    "        return sentence_embed\n",
    "\n",
    "    def score(self, sentence1, sentence2, idf=None):\n",
    "        # Cosine similarity: use np.dot & np.linalg.norm \n",
    "        embed1 = self.encode(sentence1)\n",
    "        embed2 = self.encode(sentence2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = np.dot(embed1,embed2) / (np.linalg.norm(embed1)*np.linalg.norm(embed2))\n",
    "        \n",
    "        return cosine_similarity\n",
    "    \n",
    "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
    "        # Return most similar sentences\n",
    "        scores = []  \n",
    "        \n",
    "        # Build a list of cosine similarities between the given sentence and the other sentences\n",
    "        for sentence_prime in sentences:\n",
    "            scores.append(self.score(sentence,sentence_prime))\n",
    "            \n",
    "        # Returns the indices that would sort the list of scores in the descending order\n",
    "        scores = np.array(scores)\n",
    "        scores_index_sorted = np.argsort(-scores)    \n",
    "        \n",
    "        # List of the k sentences with the most similar vector embeddings, from most similar to least similar\n",
    "        k_most_similar = [sentences[scores_index_sorted[i]] for i in range(1,k+1)]\n",
    "        \n",
    "        \n",
    "        return k_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "\n",
      "\tAverage of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.7065220648251475\n",
      "1 smiling african american boy . \n",
      "1) 2 woman dancing while pointing . \n",
      "2) 5 women and 1 man are smiling for the camera . \n",
      "3) a small boy following 4 geese . \n",
      "4) 2 female babies eating chips . \n",
      "5) a young boy and 2 girls open christmas presents . \n",
      "\n",
      "\tidf weighted average of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.7065220648251475\n",
      "a young boy and 2 girls open christmas presents . \n",
      "1) a small boy following 4 geese . \n",
      "2) a woman walking with 4 kids . \n",
      "3) 2 older women and a young girl with a red bike . \n",
      "4) two girls and a boy rollerskating . \n",
      "5) a youthful race against 2 asian kids . \n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "filepath = PATH_TO_DATA / 'sentences.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    sentences = [line.strip('\\n') for line in f]\n",
    "\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('\\n\\tAverage of word embeddings')\n",
    "sentence1 = sentences[7]\n",
    "sentence2 = sentences[13]\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2))\n",
    "sentence = sentences[10]\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
    "print(sentence)\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = sentence2vec.build_idf(sentences)\n",
    "\n",
    "print('\\n\\tidf weighted average of word embeddings')\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2, idf))\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
    "print(sentence)\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualWordAligner:\n",
    "    \n",
    "    def __init__(self, fr_word2vec, en_word2vec):\n",
    "        self.fr_word2vec = fr_word2vec\n",
    "        self.en_word2vec = en_word2vec\n",
    "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
    "        \n",
    "    def get_aligned_fr_embeddings(self):\n",
    "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
    "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
    "        identical_words = []   \n",
    "        \n",
    "        # Build a list containing the identical character strings\n",
    "        for word in fr_word2vec.words:\n",
    "            if word in en_word2vec.words:\n",
    "                identical_words.append(word)\n",
    "        \n",
    "        \n",
    "        # Build Embeddings matrix (of identical words)\n",
    "        m = len(identical_words)\n",
    "        n = len(fr_word2vec.encode(identical_words[0]))\n",
    "        X = np.zeros((n,m))\n",
    "        Y = np.zeros((n,m))\n",
    "        \n",
    "        \n",
    "        for idx, word in enumerate(identical_words):\n",
    "                X[:,idx] = self.fr_word2vec.encode(word)\n",
    "                Y[:,idx] = self.en_word2vec.encode(word)\n",
    "                \n",
    "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
    "        \n",
    "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
    "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
    "        U, S, V = np.linalg.svd(Y.dot(X.T))\n",
    "        W_optimal = U.dot(V)\n",
    "    \n",
    "    \n",
    "        assert W_optimal.shape == (300, 300)\n",
    "        \n",
    "       \n",
    "        return np.matmul(fr_word2vec.embeddings, W_optimal.transpose())\n",
    "        \n",
    "    def get_closest_english_words(self, fr_word, k=3):\n",
    "        # 3 - Return the top k English nearest neighbors to the input French word\n",
    "        \n",
    "        index = self.fr_word2vec.word2id[fr_word]\n",
    "        aligned_fr_word = self.aligned_fr_embeddings[index,:]   # Aligned embedding of fr_word\n",
    "        scores = []\n",
    "        \n",
    "        \n",
    "        # Build a list of euclidean distances between the aligned embedding of fr_word and en_words\n",
    "        for en_word_embedding in en_word2vec.embeddings:\n",
    "            cosine_similarity = np.dot(en_word_embedding,aligned_fr_word) / (np.linalg.norm(en_word_embedding)*np.linalg.norm(aligned_fr_word))\n",
    "            scores.append(cosine_similarity)\n",
    "         \n",
    "        \n",
    "        # Get the indices that would sort the list of scores in the descending order\n",
    "        scores = np.array(scores)\n",
    "        scores_index_sorted = np.argsort(-scores)\n",
    "        \n",
    "        # List of the k en_words with the closest vector embedding, from closest to least similar\n",
    "        k_nearest_neighbors = [en_word2vec.words[scores_index_sorted[i]] for i in range(0,k+1)]\n",
    "        \n",
    "            \n",
    "        return k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n",
      "----------\n",
      "fr: \"chat\"\n",
      "en: \"cat\"\n",
      "en: \"kitten\"\n",
      "en: \"kitty\"\n",
      "en: \"pet\"\n",
      "----------\n",
      "fr: \"chien\"\n",
      "en: \"dog\"\n",
      "en: \"cat\"\n",
      "en: \"pet\"\n",
      "en: \"pup\"\n",
      "----------\n",
      "fr: \"voiture\"\n",
      "en: \"car\"\n",
      "en: \"vehicle\"\n",
      "en: \"automobile\"\n",
      "en: \"motorbike\"\n",
      "----------\n",
      "fr: \"zut\"\n",
      "en: \"oops\"\n",
      "en: \"Ah\"\n",
      "en: \"ah\"\n",
      "en: \"nope\"\n"
     ]
    }
   ],
   "source": [
    "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
    "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
    "k = 3\n",
    "for fr_word in fr_words:\n",
    "    print('-' * 10)\n",
    "    print(f'fr: \"{fr_word}\"')\n",
    "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
    "    for en_word in en_words:\n",
    "        print(f'en: \"{en_word}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "train_filepath = PATH_TO_DATA / 'SST/stsa.fine.train'\n",
    "dev_filepath = PATH_TO_DATA / 'SST/stsa.fine.dev'\n",
    "test_filepath = PATH_TO_DATA / 'SST/stsa.fine.test.X'\n",
    "\n",
    "# Process file\n",
    "def process(file):\n",
    "    \n",
    "    sentences =[]\n",
    "    labels = []\n",
    "    for line in file:\n",
    "        num = line.split()\n",
    "        labels.append(int(num[0]))\n",
    "        sentence = \" \" \n",
    "        sentence = sentence.join(num[1:-1])\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        \n",
    "    return sentences, labels\n",
    "\n",
    "# Process file (To process test file)\n",
    "def process_test(file):\n",
    "    \n",
    "    sentences =[]\n",
    "    labels = []\n",
    "    for line in file:\n",
    "        num = line.split()\n",
    "        sentence = \" \"\n",
    "        sentence = sentence.join(num[0:-1])\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        \n",
    "    return sentences\n",
    "\n",
    "# Process Data\n",
    "file_train = open(train_filepath, 'r')\n",
    "train_sentences, train_labels = process(file_train)\n",
    "file_train.close()\n",
    "file_dev = open(dev_filepath, 'r')\n",
    "dev_sentences, dev_labels = process(file_dev)\n",
    "file_dev.close()\n",
    "file_test = open(test_filepath, 'r')\n",
    "test_sentences = process_test(file_test)\n",
    "file_test.close()\n",
    "file_test = open(test_filepath, 'r')\n",
    "test_sentences_copy = process_test(file_test)\n",
    "file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "out_indexes = []   # List of index where the associated sentence(in the test set) contains no words in the\n",
    "                    # vocabulary and thus has no valid sentence embedding\n",
    "\n",
    "\n",
    "# Get rid of the sentences containing no words in the vocabulary\n",
    "# Train set\n",
    "for idx,sentence in enumerate(train_sentences):\n",
    "    if np.linalg.norm(sentence2vec.encode(sentence)) == 0:\n",
    "        del train_sentences[idx]\n",
    "        del train_labels[idx]\n",
    "\n",
    "# Evaluation set\n",
    "for idx,sentence in enumerate(dev_sentences):\n",
    "    if np.linalg.norm(sentence2vec.encode(sentence)) == 0:\n",
    "        del dev_sentences[idx]\n",
    "        del dev_labels[idx]\n",
    "    \n",
    "        \n",
    "# Test set\n",
    "corr=0\n",
    "for idx,sentence in enumerate(test_sentences_copy):\n",
    "    if np.linalg.norm(sentence2vec.encode(sentence)) == 0:\n",
    "        test_sentences.pop(idx-corr)\n",
    "        out_indexes.append(idx)\n",
    "        corr += 1\n",
    "       \n",
    "    \n",
    "\n",
    "# Build the idf dictionnary with sentences from train, eval and test set\n",
    "sentences = [*train_sentences, *dev_sentences, *test_sentences]\n",
    "idf = sentence2vec.build_idf(sentences)\n",
    "\n",
    "train_idf_embeddings = []\n",
    "dev_idf_embeddings = []\n",
    "train_embeddings = []\n",
    "dev_embeddings = []\n",
    "test_idf_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "# Build the list of sentence embeddings with idf\n",
    "# Train set\n",
    "for sentence in train_sentences:\n",
    "    train_idf_embeddings.append(sentence2vec.encode(sentence,idf))\n",
    "\n",
    "# Evaluation set\n",
    "for sentence in dev_sentences:\n",
    "    dev_idf_embeddings.append(sentence2vec.encode(sentence,idf))\n",
    "\n",
    "# Test set\n",
    "for sentence in test_sentences:\n",
    "    test_idf_embeddings.append(sentence2vec.encode(sentence,idf))\n",
    "    \n",
    "# Build the list of sentence embeddings without idf\n",
    "# Train set\n",
    "for sentence in train_sentences:\n",
    "    train_embeddings.append(sentence2vec.encode(sentence))\n",
    "    \n",
    "# Evaluation set    \n",
    "for sentence in dev_sentences:\n",
    "    dev_embeddings.append(sentence2vec.encode(sentence))\n",
    "    \n",
    "# Test set    \n",
    "for sentence in test_sentences:\n",
    "    test_embeddings.append(sentence2vec.encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without IDF weights:\n",
      "\n",
      "Training accuracy with c = 0.05: 27.21%\n",
      "Validation accuracy with c = 0.05: 25.34%\n",
      "\n",
      "Training accuracy with c = 0.10: 27.82%\n",
      "Validation accuracy with c = 0.10: 26.70%\n",
      "\n",
      "Training accuracy with c = 0.15: 32.27%\n",
      "Validation accuracy with c = 0.15: 31.70%\n",
      "\n",
      "Training accuracy with c = 0.20: 33.83%\n",
      "Validation accuracy with c = 0.20: 33.24%\n",
      "\n",
      "Training accuracy with c = 0.25: 35.36%\n",
      "Validation accuracy with c = 0.25: 34.42%\n",
      "\n",
      "Training accuracy with c = 0.30: 36.90%\n",
      "Validation accuracy with c = 0.30: 35.06%\n",
      "\n",
      "Training accuracy with c = 0.35: 38.03%\n",
      "Validation accuracy with c = 0.35: 35.79%\n",
      "\n",
      "Training accuracy with c = 0.40: 38.68%\n",
      "Validation accuracy with c = 0.40: 36.06%\n",
      "\n",
      "Training accuracy with c = 0.45: 39.61%\n",
      "Validation accuracy with c = 0.45: 37.06%\n",
      "\n",
      "Training accuracy with c = 0.50: 40.09%\n",
      "Validation accuracy with c = 0.50: 37.60%\n",
      "\n",
      "Training accuracy with c = 0.55: 40.44%\n",
      "Validation accuracy with c = 0.55: 38.33%\n",
      "\n",
      "Training accuracy with c = 0.60: 41.04%\n",
      "Validation accuracy with c = 0.60: 38.87%\n",
      "\n",
      "Training accuracy with c = 0.65: 41.46%\n",
      "Validation accuracy with c = 0.65: 39.06%\n",
      "\n",
      "Training accuracy with c = 0.70: 41.74%\n",
      "Validation accuracy with c = 0.70: 39.24%\n",
      "\n",
      "Training accuracy with c = 0.75: 42.00%\n",
      "Validation accuracy with c = 0.75: 39.33%\n",
      "\n",
      "Training accuracy with c = 0.80: 42.50%\n",
      "Validation accuracy with c = 0.80: 39.15%\n",
      "\n",
      "Training accuracy with c = 0.85: 42.67%\n",
      "Validation accuracy with c = 0.85: 38.78%\n",
      "\n",
      "Training accuracy with c = 0.90: 42.72%\n",
      "Validation accuracy with c = 0.90: 38.60%\n",
      "\n",
      "Training accuracy with c = 0.95: 42.89%\n",
      "Validation accuracy with c = 0.95: 39.24%\n",
      "\n",
      "Training accuracy with c = 1.00: 43.14%\n",
      "Validation accuracy with c = 1.00: 39.51%\n",
      "\n",
      "Training accuracy with c = 1.05: 43.36%\n",
      "Validation accuracy with c = 1.05: 39.78%\n",
      "\n",
      "Training accuracy with c = 1.10: 43.49%\n",
      "Validation accuracy with c = 1.10: 40.05%\n",
      "\n",
      "Training accuracy with c = 1.15: 43.53%\n",
      "Validation accuracy with c = 1.15: 39.87%\n",
      "\n",
      "Training accuracy with c = 1.20: 43.70%\n",
      "Validation accuracy with c = 1.20: 39.87%\n",
      "\n",
      "Training accuracy with c = 1.25: 43.80%\n",
      "Validation accuracy with c = 1.25: 39.87%\n",
      "\n",
      "Training accuracy with c = 1.30: 43.87%\n",
      "Validation accuracy with c = 1.30: 39.06%\n",
      "\n",
      "Training accuracy with c = 1.35: 44.05%\n",
      "Validation accuracy with c = 1.35: 39.33%\n",
      "\n",
      "Training accuracy with c = 1.40: 44.14%\n",
      "Validation accuracy with c = 1.40: 39.60%\n",
      "\n",
      "Training accuracy with c = 1.45: 44.20%\n",
      "Validation accuracy with c = 1.45: 39.87%\n",
      "\n",
      "Training accuracy with c = 1.50: 44.24%\n",
      "Validation accuracy with c = 1.50: 39.78%\n",
      "\n",
      "Training accuracy with c = 1.55: 44.38%\n",
      "Validation accuracy with c = 1.55: 39.78%\n",
      "\n",
      "Training accuracy with c = 1.60: 44.45%\n",
      "Validation accuracy with c = 1.60: 39.60%\n",
      "\n",
      "Training accuracy with c = 1.65: 44.39%\n",
      "Validation accuracy with c = 1.65: 39.42%\n",
      "\n",
      "Training accuracy with c = 1.70: 44.37%\n",
      "Validation accuracy with c = 1.70: 39.42%\n",
      "\n",
      "Training accuracy with c = 1.75: 44.46%\n",
      "Validation accuracy with c = 1.75: 39.24%\n",
      "\n",
      "Training accuracy with c = 1.80: 44.54%\n",
      "Validation accuracy with c = 1.80: 38.96%\n",
      "\n",
      "Training accuracy with c = 1.85: 44.65%\n",
      "Validation accuracy with c = 1.85: 38.78%\n",
      "\n",
      "Training accuracy with c = 1.90: 44.62%\n",
      "Validation accuracy with c = 1.90: 38.96%\n",
      "\n",
      "Training accuracy with c = 1.95: 44.74%\n",
      "Validation accuracy with c = 1.95: 38.96%\n",
      "\n",
      "Training accuracy with c = 2.00: 44.78%\n",
      "Validation accuracy with c = 2.00: 38.60%\n",
      "\n",
      "With IDF weights:\n",
      "\n",
      "Training accuracy with c = 0.05: 36.17%\n",
      "Validation accuracy with c = 0.05: 33.70%\n",
      "\n",
      "Training accuracy with c = 0.10: 41.15%\n",
      "Validation accuracy with c = 0.10: 39.87%\n",
      "\n",
      "Training accuracy with c = 0.15: 42.78%\n",
      "Validation accuracy with c = 0.15: 41.14%\n",
      "\n",
      "Training accuracy with c = 0.20: 43.62%\n",
      "Validation accuracy with c = 0.20: 41.14%\n",
      "\n",
      "Training accuracy with c = 0.25: 44.10%\n",
      "Validation accuracy with c = 0.25: 41.24%\n",
      "\n",
      "Training accuracy with c = 0.30: 44.59%\n",
      "Validation accuracy with c = 0.30: 41.60%\n",
      "\n",
      "Training accuracy with c = 0.35: 44.75%\n",
      "Validation accuracy with c = 0.35: 41.33%\n",
      "\n",
      "Training accuracy with c = 0.40: 45.22%\n",
      "Validation accuracy with c = 0.40: 41.51%\n",
      "\n",
      "Training accuracy with c = 0.45: 45.28%\n",
      "Validation accuracy with c = 0.45: 42.14%\n",
      "\n",
      "Training accuracy with c = 0.50: 45.53%\n",
      "Validation accuracy with c = 0.50: 42.60%\n",
      "\n",
      "Training accuracy with c = 0.55: 45.62%\n",
      "Validation accuracy with c = 0.55: 42.60%\n",
      "\n",
      "Training accuracy with c = 0.60: 45.77%\n",
      "Validation accuracy with c = 0.60: 42.23%\n",
      "\n",
      "Training accuracy with c = 0.65: 46.04%\n",
      "Validation accuracy with c = 0.65: 41.96%\n",
      "\n",
      "Training accuracy with c = 0.70: 46.22%\n",
      "Validation accuracy with c = 0.70: 42.05%\n",
      "\n",
      "Training accuracy with c = 0.75: 46.27%\n",
      "Validation accuracy with c = 0.75: 41.87%\n",
      "\n",
      "Training accuracy with c = 0.80: 46.35%\n",
      "Validation accuracy with c = 0.80: 41.78%\n",
      "\n",
      "Training accuracy with c = 0.85: 46.47%\n",
      "Validation accuracy with c = 0.85: 41.69%\n",
      "\n",
      "Training accuracy with c = 0.90: 46.49%\n",
      "Validation accuracy with c = 0.90: 41.42%\n",
      "\n",
      "Training accuracy with c = 0.95: 46.52%\n",
      "Validation accuracy with c = 0.95: 41.14%\n",
      "\n",
      "Training accuracy with c = 1.00: 46.50%\n",
      "Validation accuracy with c = 1.00: 41.05%\n",
      "\n",
      "Training accuracy with c = 1.05: 46.59%\n",
      "Validation accuracy with c = 1.05: 40.51%\n",
      "\n",
      "Training accuracy with c = 1.10: 46.57%\n",
      "Validation accuracy with c = 1.10: 40.96%\n",
      "\n",
      "Training accuracy with c = 1.15: 46.70%\n",
      "Validation accuracy with c = 1.15: 41.05%\n",
      "\n",
      "Training accuracy with c = 1.20: 46.82%\n",
      "Validation accuracy with c = 1.20: 41.24%\n",
      "\n",
      "Training accuracy with c = 1.25: 46.79%\n",
      "Validation accuracy with c = 1.25: 41.05%\n",
      "\n",
      "Training accuracy with c = 1.30: 46.76%\n",
      "Validation accuracy with c = 1.30: 40.96%\n",
      "\n",
      "Training accuracy with c = 1.35: 46.71%\n",
      "Validation accuracy with c = 1.35: 40.87%\n",
      "\n",
      "Training accuracy with c = 1.40: 46.61%\n",
      "Validation accuracy with c = 1.40: 40.96%\n",
      "\n",
      "Training accuracy with c = 1.45: 46.65%\n",
      "Validation accuracy with c = 1.45: 40.87%\n",
      "\n",
      "Training accuracy with c = 1.50: 46.65%\n",
      "Validation accuracy with c = 1.50: 40.87%\n",
      "\n",
      "Training accuracy with c = 1.55: 46.79%\n",
      "Validation accuracy with c = 1.55: 40.69%\n",
      "\n",
      "Training accuracy with c = 1.60: 46.78%\n",
      "Validation accuracy with c = 1.60: 40.42%\n",
      "\n",
      "Training accuracy with c = 1.65: 46.77%\n",
      "Validation accuracy with c = 1.65: 40.42%\n",
      "\n",
      "Training accuracy with c = 1.70: 46.81%\n",
      "Validation accuracy with c = 1.70: 40.33%\n",
      "\n",
      "Training accuracy with c = 1.75: 46.77%\n",
      "Validation accuracy with c = 1.75: 40.51%\n",
      "\n",
      "Training accuracy with c = 1.80: 46.75%\n",
      "Validation accuracy with c = 1.80: 40.42%\n",
      "\n",
      "Training accuracy with c = 1.85: 46.72%\n",
      "Validation accuracy with c = 1.85: 40.51%\n",
      "\n",
      "Training accuracy with c = 1.90: 46.70%\n",
      "Validation accuracy with c = 1.90: 40.42%\n",
      "\n",
      "Training accuracy with c = 1.95: 46.66%\n",
      "Validation accuracy with c = 1.95: 40.51%\n",
      "\n",
      "Training accuracy with c = 2.00: 46.72%\n",
      "Validation accuracy with c = 2.00: 40.60%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
    "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# Convert list to array to use scikit-learn\n",
    "train_embeddings = np.stack(train_embeddings, axis=0 )\n",
    "train_labels = np.array(train_labels)\n",
    "dev_embeddings = np.stack(dev_embeddings,axis=0)\n",
    "dev_labels = np.array(dev_labels)\n",
    "\n",
    "hyp_para = np.linspace(0.05,2,40)  # parameters for tuning\n",
    "hyp_para.tolist()\n",
    "train_scores_idf = []\n",
    "train_scores = []\n",
    "dev_scores_idf = []\n",
    "dev_scores = []\n",
    "\n",
    "i = 0\n",
    "print('Without IDF weights:\\n')\n",
    "# Compute score on dev set for each regularization parameter \n",
    "for c in hyp_para:\n",
    "    LogReg = sklearn.linear_model.LogisticRegression(penalty='l1', tol= 0.0001, C=c, multi_class='auto')\n",
    "    LogReg.fit(train_embeddings, train_labels)\n",
    "    train_scores.append(LogReg.score(train_embeddings, train_labels))\n",
    "    dev_scores.append(LogReg.score(dev_embeddings,dev_labels))\n",
    "    print('Training accuracy with c = {:.2f}: {:.2f}%'.format(c, 100*train_scores[i]))\n",
    "    print('Validation accuracy with c = {:.2f}: {:.2f}%\\n'.format(c, 100*dev_scores[i]))\n",
    " \n",
    "    i += 1\n",
    "    \n",
    "print\n",
    "    \n",
    "    \n",
    "i = 0   \n",
    "print('With IDF weights:\\n')\n",
    "for c in hyp_para:\n",
    "    LogReg = sklearn.linear_model.LogisticRegression(penalty='l1', tol= 0.0001, C=c, multi_class='auto')\n",
    "    LogReg.fit(train_idf_embeddings, train_labels)\n",
    "    train_scores_idf.append(LogReg.score(train_idf_embeddings, train_labels))\n",
    "    dev_scores_idf.append(LogReg.score(dev_idf_embeddings,dev_labels))\n",
    "    print('Training accuracy with c = {:.2f}: {:.2f}%'.format(c, 100*train_scores_idf[i]))\n",
    "    print('Validation accuracy with c = {:.2f}: {:.2f}%\\n'.format(c, 100*dev_scores_idf[i]))\n",
    "    i += 1\n",
    "    \n",
    "# Note: also tried with the l2 penalty and several other tolerance rates and I got the best result with this\n",
    "# configuaration\n",
    "    \n",
    "# To array\n",
    "train_scores_idf = np.array(train_scores_idf)\n",
    "dev_scores_idf = np.array(dev_scores_idf)\n",
    "train_scores = np.array(train_scores)\n",
    "dev_scores = np.array(dev_scores)\n",
    "\n",
    "# Optimal regularization parameter\n",
    "c_idf_optimal = hyp_para[np.argmax(dev_scores_idf)]\n",
    "c_optimal = hyp_para[np.argmax(dev_scores)]\n",
    "\n",
    "# Optimal scores\n",
    "score_train_idf_optimal = np.max(train_scores_idf)\n",
    "score_dev_idf_optimal = np.max(dev_scores_idf)\n",
    "score_train_optimal = np.max(train_scores)\n",
    "score_dev_optimal = np.max(dev_scores)\n",
    "\n",
    "# Plot\n",
    "# data\n",
    "train_accuracy = [100*ele for ele in train_scores]\n",
    "train_accuracy_idf = [100*ele for ele in train_scores_idf]\n",
    "val_accuracy = [100*ele for ele in dev_scores]\n",
    "val_accuracy_idf = [100*ele for ele in dev_scores_idf]\n",
    "x_axis = hyp_para\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
    "\n",
    "# Left figure\n",
    "axs[0].plot(x_axis, train_accuracy, label='without idf')\n",
    "axs[0].plot(x_axis, train_accuracy_idf, label='with idf')\n",
    "axs[0].set_xlabel('Regularization parameter')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Train set')\n",
    "\n",
    "# Right figure\n",
    "axs[1].plot(x_axis, val_accuracy, label='without idf')\n",
    "axs[1].plot(x_axis, val_accuracy_idf , label='with idf')\n",
    "axs[1].set_xlabel('Regularization parameter')\n",
    "axs[1].set_ylabel('Accuracy (%)')\n",
    "axs[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Evaluation set')\n",
    "\n",
    "plt.savefig('Plot2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# Prediction with the best model from above\n",
    "LogReg = sklearn.linear_model.LogisticRegression(penalty='l1', tol= 0.0001, C=c_idf_optimal, multi_class='auto')\n",
    "LogReg.fit(train_idf_embeddings, train_labels)\n",
    "predictions_idf = LogReg.predict(test_idf_embeddings)\n",
    "predictions_idf = predictions_idf.tolist()\n",
    "\n",
    "# Prediction embeddings without idf weights(of sentences with valid embeddings)\n",
    "LogReg = sklearn.linear_model.LogisticRegression(penalty='l1', tol= 0.0001, C=c_optimal, multi_class='auto')\n",
    "LogReg.fit(train_embeddings, train_labels)\n",
    "predictions = LogReg.predict(test_embeddings)\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "# Add prediction of sentences containing no words in the vocabulary\n",
    "# These are the sentences containing no words in the vocab: 'anthony hopkins', 'brimful' and 'hopkins'\n",
    "# We annotated these sentences according our own opinion:\n",
    "# For 'anthony hopkins' and 'hopkins', it is quite neutral sentence but has a positive connotation as anthony\n",
    "# hopkins is a popular actor, so we assign to both the value 3\n",
    "# For 'brimful', this is clearly a positive adjective so we decided to assign the value 5\n",
    "a = [3,5,3]\n",
    "for i, idx in enumerate(out_indexes):\n",
    "    predictions_idf.insert(idx,a[i])\n",
    "    \n",
    "# In practice for large scale projects, this \"trick\" is unfeasible but since we are evaluated\n",
    "# on the accuray of our predicitons on the test set, the few sentences containing no words\n",
    "# in the vocabulary will certainly be missclassified by our model and so human annotation is more accurate.\n",
    "    \n",
    "# Transfer predictions to file\n",
    "with open('logreg_bov_y_test_sst.txt', 'w') as f:\n",
    "    for item in predictions_idf:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# Use Passive Aggressive classifier to predict labels\n",
    "pac = PassiveAggressiveClassifier(C=0.00009,max_iter=10000, random_state=0,tol=1e-3)\n",
    "pac.fit(train_idf_embeddings, train_labels)\n",
    "score_pac = pac.score(dev_idf_embeddings,dev_labels)\n",
    "predictions_pac = pac.predict(test_idf_embeddings)\n",
    "predictions_pac = predictions_pac.tolist()\n",
    "\n",
    "# Add prediction of sentences containing no words in the vocabulary, see above for more explanations\n",
    "a = [3,5,3]\n",
    "for i, idx in enumerate(out_indexes):\n",
    "    predictions_pac.insert(idx,a[i])\n",
    "    \n",
    "# Transfer predictions to file\n",
    "with open('passagg_bov_y_test_sst.txt', 'w') as f:\n",
    "    for item in predictions_idf:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "train_one_hot = []\n",
    "dev_one_hot = []\n",
    "test_one_hot = []\n",
    "length_sentences = []\n",
    "\n",
    "# Transform sentences to list of integers\n",
    "# Train set\n",
    "for i in range(len(train_sentences)):\n",
    "    train_embeddings = tf.keras.preprocessing.text.one_hot(train_sentences[i],n=50000,split=\" \")\n",
    "    train_one_hot.append(train_embeddings)\n",
    "    length_sentences.append(len(train_embeddings))\n",
    "    \n",
    "# Evaluation set\n",
    "for i in range(len(dev_sentences)):\n",
    "    dev_embeddings = tf.keras.preprocessing.text.one_hot(dev_sentences[i],n=50000,split=\" \")\n",
    "    dev_one_hot.append(dev_embeddings)\n",
    "    length_sentences.append(len(dev_embeddings))\n",
    "\n",
    "# Test set\n",
    "for i in range(len(test_sentences)):\n",
    "    test_embeddings = tf.keras.preprocessing.text.one_hot(test_sentences[i],n=50000,split=\" \")\n",
    "    test_one_hot.append(test_embeddings)\n",
    "    length_sentences.append(len(test_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# We only look at the first 20 words in each sentence (and pad to 20 if the sentence contains fewer words)\n",
    "# By setting maxlen = max of sentences length, the train matrix gets too sparse and we obtain poor results.\n",
    "maxlen = 20\n",
    "\n",
    "# Padding\n",
    "# Train set\n",
    "train_one_hot_pad = tf.keras.preprocessing.sequence.pad_sequences(train_one_hot, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n",
    "# Evaluation set\n",
    "dev_one_hot_pad = tf.keras.preprocessing.sequence.pad_sequences(dev_one_hot, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n",
    "\n",
    "# Test set\n",
    "test_one_hot_pad = tf.keras.preprocessing.sequence.pad_sequences(test_one_hot, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 64  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 50000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=maxlen, \n",
    "                    name='Word2vec'))                    # Use pre-trained embedding Word2vec\n",
    "model.add(LSTM(nhid, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Word2vec (Embedding)         (None, 20, 64)            3200000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 3,233,349\n",
      "Trainable params: 3,233,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # Use categorical Cross-Entropy\n",
    "optimizer        =  'rmsprop' # Use Root Mean Square Prop\n",
    "metrics_classif  =  ['accuracy']  # Accuracy the measure the performance on the train, validation and test sets\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8529 samples, validate on 1101 samples\n",
      "Epoch 1/12\n",
      "8529/8529 [==============================] - 21s 3ms/sample - loss: 1.5778 - acc: 0.2694 - val_loss: 1.5785 - val_acc: 0.2534\n",
      "Epoch 2/12\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.5709 - acc: 0.2715 - val_loss: 1.5716 - val_acc: 0.2534\n",
      "Epoch 3/12\n",
      "8529/8529 [==============================] - 23s 3ms/sample - loss: 1.5652 - acc: 0.2730 - val_loss: 1.5723 - val_acc: 0.2534\n",
      "Epoch 4/12\n",
      "8529/8529 [==============================] - 22s 3ms/sample - loss: 1.5627 - acc: 0.2746 - val_loss: 1.5659 - val_acc: 0.2534\n",
      "Epoch 5/12\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.5436 - acc: 0.2805 - val_loss: 1.5499 - val_acc: 0.3070\n",
      "Epoch 6/12\n",
      "8529/8529 [==============================] - 22s 3ms/sample - loss: 1.5163 - acc: 0.3080 - val_loss: 1.5317 - val_acc: 0.3179\n",
      "Epoch 7/12\n",
      "8529/8529 [==============================] - 21s 2ms/sample - loss: 1.4942 - acc: 0.3269 - val_loss: 1.4976 - val_acc: 0.3315\n",
      "Epoch 8/12\n",
      "8529/8529 [==============================] - 21s 3ms/sample - loss: 1.4594 - acc: 0.3515 - val_loss: 1.4814 - val_acc: 0.3533\n",
      "Epoch 9/12\n",
      "8529/8529 [==============================] - 23s 3ms/sample - loss: 1.4235 - acc: 0.3758 - val_loss: 1.4479 - val_acc: 0.3615\n",
      "Epoch 10/12\n",
      "8529/8529 [==============================] - 21s 2ms/sample - loss: 1.3851 - acc: 0.3913 - val_loss: 1.4317 - val_acc: 0.3678\n",
      "Epoch 11/12\n",
      "8529/8529 [==============================] - 22s 3ms/sample - loss: 1.3419 - acc: 0.4121 - val_loss: 1.4208 - val_acc: 0.3860\n",
      "Epoch 12/12\n",
      "8529/8529 [==============================] - 22s 3ms/sample - loss: 1.3017 - acc: 0.4382 - val_loss: 1.4422 - val_acc: 0.3787\n"
     ]
    }
   ],
   "source": [
    "# 5 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "bs = 32   # batch size\n",
    "n_epochs = 12 # number of epochs, set to 20 to get the plot\n",
    "\n",
    "# Data\n",
    "x_train = train_one_hot_pad\n",
    "x_dev = dev_one_hot_pad\n",
    "\n",
    "\n",
    "# Transform vector labels to one-hot encodings of the labels\n",
    "y_train = np.zeros((len(train_one_hot_pad[:,0]),n_classes), dtype='int')\n",
    "y_dev = np.zeros((len(dev_one_hot_pad[:,0]),n_classes),dtype='int' )\n",
    "\n",
    "for i in range(len(train_one_hot_pad[:,0])):\n",
    "    y_train[i,train_labels[i]] = 1\n",
    "    \n",
    "for i in range(len(dev_one_hot_pad[:,0])):\n",
    "    y_dev[i,dev_labels[i]] = 1\n",
    "\n",
    "# Train the model   \n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev, y_dev))\n",
    "\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# data\n",
    "train_accuracy = [i * 100 for i in history.history['acc']]\n",
    "train_loss = history.history['loss']\n",
    "val_accuracy = [i * 100 for i in history.history['val_acc']]\n",
    "val_loss = history.history['val_loss']\n",
    "x_axis = np.arange(1, 21, 1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
    "\n",
    "# Left figure\n",
    "axs[0].plot(x_axis, train_accuracy, label='Train accuracy')\n",
    "axs[0].plot(x_axis, val_accuracy, label='Eval accuracy')\n",
    "axs[0].set_xlabel('Number of epochs')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Evolution of Accuracy during Training')\n",
    "\n",
    "# Right figure\n",
    "axs[1].plot(x_axis, train_loss, label='Train Loss')\n",
    "axs[1].plot(x_axis, val_loss, label='Eval Loss')\n",
    "axs[1].set_xlabel('Number of epochs')\n",
    "axs[1].set_ylabel('Cross-Entropy Loss')\n",
    "axs[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Evolution of Cross-Entropy Loss during Training')\n",
    "\n",
    "plt.savefig('Plot1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# Data\n",
    "x_test = test_one_hot_pad\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict_classes(x_test)\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "\n",
    "# Transfer predictions to file\n",
    "with open('logreg_lstm_y_test_sst.txt', 'w') as f:\n",
    "    for item in predictions:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Philippe-\n",
      "[nltk_data]     Ganshof/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "train_processed_sentences = []\n",
    "dev_processed_sentences = []\n",
    "test_processed_sentences = []\n",
    "train_one_hot_pro = []\n",
    "dev_one_hot_pro = []\n",
    "test_one_hot_pro = []\n",
    "length_sentences_pro = []\n",
    "\n",
    "\n",
    "\n",
    "# Pre-processing: Get rid of common words\n",
    "# Train set\n",
    "for sentence in train_sentences:\n",
    "    sent = nltk.sent_tokenize(sentence)\n",
    "    sent_words = [nltk.word_tokenize(ele) for ele in sent][0]\n",
    "    sent_words2 = [w for w in sent_words if w not in stopwords.words('english')]\n",
    "    pro_sent = \" \"\n",
    "    pro_sent = pro_sent.join(sent_words2)\n",
    "    train_processed_sentences.append(pro_sent)\n",
    "\n",
    "# Eval set\n",
    "for sentence in dev_sentences:\n",
    "    sent = nltk.sent_tokenize(sentence)\n",
    "    sent_words = [nltk.word_tokenize(ele) for ele in sent][0]\n",
    "    sent_words2 = [w for w in sent_words if w not in stopwords.words('english')]\n",
    "    pro_sent = \" \"\n",
    "    pro_sent = pro_sent.join(sent_words2)\n",
    "    dev_processed_sentences.append(pro_sent)\n",
    "    \n",
    "# Test set\n",
    "for sentence in test_sentences:\n",
    "    sent = nltk.sent_tokenize(sentence)\n",
    "    sent_words = [nltk.word_tokenize(ele) for ele in sent][0]\n",
    "    sent_words2 = [w for w in sent_words if w not in stopwords.words('english')]\n",
    "    pro_sent = \" \"\n",
    "    pro_sent = pro_sent.join(sent_words2)\n",
    "    test_processed_sentences.append(pro_sent)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Transform sentences to list of integers\n",
    "# Train set\n",
    "for i in range(len(train_sentences)):\n",
    "    train_embeddings = tf.keras.preprocessing.text.one_hot(train_processed_sentences[i],n=50000,split=\" \")\n",
    "    train_one_hot_pro.append(train_embeddings)\n",
    "    length_sentences_pro.append(len(train_embeddings))\n",
    "    \n",
    "# Evaluation set\n",
    "for i in range(len(dev_sentences)):\n",
    "    dev_embeddings = tf.keras.preprocessing.text.one_hot(dev_processed_sentences[i],n=50000,split=\" \")\n",
    "    dev_one_hot_pro.append(dev_embeddings)\n",
    "    length_sentences_pro.append(len(dev_embeddings))\n",
    "    \n",
    "# Test set\n",
    "for i in range(len(test_sentences)):\n",
    "    test_embeddings = tf.keras.preprocessing.text.one_hot(test_processed_sentences[i],n=50000,split=\" \")\n",
    "    test_one_hot_pro.append(test_embeddings)\n",
    "    length_sentences_pro.append(len(test_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "embed_dim  = 64  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 50000  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "# Build my own model\n",
    "# Architecture\n",
    "mymodel = Sequential()\n",
    "mymodel.add(Embedding(vocab_size, embed_dim, \n",
    "                    name='Word2vec'))                    # Use pre-trained embedding Word2vec\n",
    "mymodel.add(Bidirectional(LSTM(nhid, dropout=0.7, recurrent_dropout=0.7)))\n",
    "mymodel.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "# Classifier/Optimizer/Metrics\n",
    "loss_classif     =  'categorical_crossentropy' # Use categorical Cross-Entropy\n",
    "optimizer        =  'rmsprop' # Use Root Mean Square Prop\n",
    "metrics_classif  =  ['accuracy']  # Accuracy the measure the performance on the train, validation and test sets\n",
    "\n",
    "mymodel.compile(loss=loss_classif, optimizer=optimizer, metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 12\n",
    "\n",
    "# Padding\n",
    "# Train set\n",
    "train_one_hot_pad_pro = tf.keras.preprocessing.sequence.pad_sequences(train_one_hot_pro, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n",
    "# Evaluation set\n",
    "dev_one_hot_pad_pro = tf.keras.preprocessing.sequence.pad_sequences(dev_one_hot_pro, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n",
    "\n",
    "# Test set\n",
    "test_one_hot_pad_pro = tf.keras.preprocessing.sequence.pad_sequences(test_one_hot_pro, \n",
    "                                                                       maxlen=maxlen,dtype='int32',\n",
    "                                                                       padding='post', truncating= 'post',\n",
    "                                                                       value= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8529 samples, validate on 1101 samples\n",
      "Epoch 1/9\n",
      "8529/8529 [==============================] - 25s 3ms/sample - loss: 1.5734 - acc: 0.2708 - val_loss: 1.5695 - val_acc: 0.2534\n",
      "Epoch 2/9\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.5540 - acc: 0.2928 - val_loss: 1.5375 - val_acc: 0.3252\n",
      "Epoch 3/9\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.4624 - acc: 0.3614 - val_loss: 1.4243 - val_acc: 0.3588\n",
      "Epoch 4/9\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.3509 - acc: 0.4091 - val_loss: 1.3931 - val_acc: 0.3787\n",
      "Epoch 5/9\n",
      "8529/8529 [==============================] - 21s 2ms/sample - loss: 1.2703 - acc: 0.4257 - val_loss: 1.3961 - val_acc: 0.3778\n",
      "Epoch 6/9\n",
      "8529/8529 [==============================] - 21s 2ms/sample - loss: 1.2073 - acc: 0.4612 - val_loss: 1.4131 - val_acc: 0.3933\n",
      "Epoch 7/9\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.1474 - acc: 0.4915 - val_loss: 1.4439 - val_acc: 0.3942\n",
      "Epoch 8/9\n",
      "8529/8529 [==============================] - 22s 3ms/sample - loss: 1.0892 - acc: 0.5356 - val_loss: 1.4962 - val_acc: 0.4033\n",
      "Epoch 9/9\n",
      "8529/8529 [==============================] - 20s 2ms/sample - loss: 1.0344 - acc: 0.5769 - val_loss: 1.5569 - val_acc: 0.4060\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "bs = 32   # batch size\n",
    "n_epochs = 9 # number of epochs\n",
    "\n",
    "\n",
    "# Data\n",
    "x_train_pro = train_one_hot_pad_pro\n",
    "x_dev_pro = dev_one_hot_pad_pro\n",
    "\n",
    "\n",
    "# Train the model   \n",
    "myhistory = mymodel.fit(x_train_pro, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_dev_pro, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x_test = test_one_hot_pad_pro\n",
    "\n",
    "# Predictions\n",
    "predictions = mymodel.predict_classes(x_test)\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "\n",
    "# Transfer predictions to file\n",
    "with open('logreg_Bilstm_y_test_sst.txt', 'w') as f:\n",
    "    for item in predictions:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
